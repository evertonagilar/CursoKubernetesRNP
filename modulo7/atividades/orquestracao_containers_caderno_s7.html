<!DOCTYPE html>
<html lang="pt_BR">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.17">
<title>Sessão 7: Armazenamento no Kubernetes</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
pre.rouge table td { padding: 5px; }
pre.rouge table pre { margin: 0; }
pre.rouge .cm {
  color: #999988;
  font-style: italic;
}
pre.rouge .cp {
  color: #999999;
  font-weight: bold;
}
pre.rouge .c1 {
  color: #999988;
  font-style: italic;
}
pre.rouge .cs {
  color: #999999;
  font-weight: bold;
  font-style: italic;
}
pre.rouge .c, pre.rouge .ch, pre.rouge .cd, pre.rouge .cpf {
  color: #999988;
  font-style: italic;
}
pre.rouge .err {
  color: #a61717;
  background-color: #e3d2d2;
}
pre.rouge .gd {
  color: #000000;
  background-color: #ffdddd;
}
pre.rouge .ge {
  color: #000000;
  font-style: italic;
}
pre.rouge .gr {
  color: #aa0000;
}
pre.rouge .gh {
  color: #999999;
}
pre.rouge .gi {
  color: #000000;
  background-color: #ddffdd;
}
pre.rouge .go {
  color: #888888;
}
pre.rouge .gp {
  color: #555555;
}
pre.rouge .gs {
  font-weight: bold;
}
pre.rouge .gu {
  color: #aaaaaa;
}
pre.rouge .gt {
  color: #aa0000;
}
pre.rouge .kc {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kd {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kn {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kp {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kr {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kt {
  color: #445588;
  font-weight: bold;
}
pre.rouge .k, pre.rouge .kv {
  color: #000000;
  font-weight: bold;
}
pre.rouge .mf {
  color: #009999;
}
pre.rouge .mh {
  color: #009999;
}
pre.rouge .il {
  color: #009999;
}
pre.rouge .mi {
  color: #009999;
}
pre.rouge .mo {
  color: #009999;
}
pre.rouge .m, pre.rouge .mb, pre.rouge .mx {
  color: #009999;
}
pre.rouge .sa {
  color: #000000;
  font-weight: bold;
}
pre.rouge .sb {
  color: #d14;
}
pre.rouge .sc {
  color: #d14;
}
pre.rouge .sd {
  color: #d14;
}
pre.rouge .s2 {
  color: #d14;
}
pre.rouge .se {
  color: #d14;
}
pre.rouge .sh {
  color: #d14;
}
pre.rouge .si {
  color: #d14;
}
pre.rouge .sx {
  color: #d14;
}
pre.rouge .sr {
  color: #009926;
}
pre.rouge .s1 {
  color: #d14;
}
pre.rouge .ss {
  color: #990073;
}
pre.rouge .s, pre.rouge .dl {
  color: #d14;
}
pre.rouge .na {
  color: #008080;
}
pre.rouge .bp {
  color: #999999;
}
pre.rouge .nb {
  color: #0086B3;
}
pre.rouge .nc {
  color: #445588;
  font-weight: bold;
}
pre.rouge .no {
  color: #008080;
}
pre.rouge .nd {
  color: #3c5d5d;
  font-weight: bold;
}
pre.rouge .ni {
  color: #800080;
}
pre.rouge .ne {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nf, pre.rouge .fm {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nl {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nn {
  color: #555555;
}
pre.rouge .nt {
  color: #000080;
}
pre.rouge .vc {
  color: #008080;
}
pre.rouge .vg {
  color: #008080;
}
pre.rouge .vi {
  color: #008080;
}
pre.rouge .nv, pre.rouge .vm {
  color: #008080;
}
pre.rouge .ow {
  color: #000000;
  font-weight: bold;
}
pre.rouge .o {
  color: #000000;
  font-weight: bold;
}
pre.rouge .w {
  color: #bbbbbb;
}
pre.rouge {
  background-color: #f8f8f8;
}
</style>
</head>
<body class="article toc2 toc-left">
<div id="header">
<div id="toc" class="toc2">
<div id="toctitle">Índice</div>
<ul class="sectlevel1">
<li><a href="#_sessão_7_armazenamento_no_kubernetes">Sessão 7: Armazenamento no Kubernetes</a>
<ul class="sectlevel2">
<li><a href="#_1_persistência_de_arquivos">1) Persistência de arquivos</a></li>
<li><a href="#_2_volumes_persistentes_e_claims">2) Volumes persistentes e <em>claims</em></a></li>
<li><a href="#_3_storage_classes">3) <em>Storage Classes</em></a></li>
<li><a href="#_3_1_provisionamento_manual">3.1) Provisionamento manual</a></li>
<li><a href="#_3_2_provisionamento_dinâmico">3.2) Provisionamento dinâmico</a></li>
<li><a href="#_3_3_múltiplas_utilizações_concorrentes_de_volumes">3.3) Múltiplas utilizações concorrentes de volumes</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="paragraph">
<p><span class="image"><img src="./img/logoRNP.png" alt="logoRNP" width="150" height="60"></span></p>
</div>
<div class="sect1">
<h2 id="_sessão_7_armazenamento_no_kubernetes">Sessão 7: Armazenamento no Kubernetes</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_1_persistência_de_arquivos">1) Persistência de arquivos</h3>
<div class="olist loweralpha">
<ol class="loweralpha">
<li>
<p>Crie um pod com o nome <code>writer</code>, usando a imagem <code>busybox</code> e executando o comando <code>sleep 3600</code>.</p>
<div class="paragraph">
<p>A seguir, execute um <em>shell</em> interativo no pod e crie o diretório novo <code>/data</code>. Dentro dele, crie um arquivo com o nome <code>hello</code> e conteúdo <code>world</code>.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Vamos lá: primeiro, criamos o pod.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl run writer --image=busybox -- sleep 3600</strong>
pod/writer created</pre>
</div>
</div>
<div class="paragraph">
<p>Para criar o arquivo, iremos acessar o pod com um <em>shell</em> interativo:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it writer -- /bin/sh</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Verifique que o contexto foi alterado corretamente:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>/ # whoami ; hostname</strong>
root
writer</pre>
</div>
</div>
<div class="paragraph">
<p>Note que não existe ainda nenhum diretório <code>data</code> na raiz do sistema de arquivos. Vamos criá-lo.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>/ # ls</strong>
bin    etc    lib    proc   sys    usr
dev    home   lib64  root   tmp    var</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>/ # mkdir /data</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Para criar o arquivo <code>hello</code>, basta redirecionar a saída do comando <code>echo</code>, assim:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>/ # echo world > /data/hello</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Vejamos se funcionou:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>/ # cat /data/hello</strong>
world</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Agora, remova o pod <code>writer</code>. O conteúdo do arquivo <code>/data/hello</code> permanece acessível?</p>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Antes de remover o pod, devemos sair do <em>shell</em> interativo dentro do pod <code>writer</code>. Verifique que o contexto foi alterado com sucesso.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># whoami ; hostname</strong>
root
s2-master-1</pre>
</div>
</div>
<div class="paragraph">
<p>Agora, remova o pod.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pod writer</strong>
pod "writer" deleted</pre>
</div>
</div>
<div class="paragraph">
<p>O conteúdo está inacessível, pois ele não foi armazenado permanentemente em nenhum lugar. De fato, o pod em si não pode ser acessado, uma vez que tenha sido removido.</p>
</div>
</div>
</details>
</li>
<li>
<p>Vamos solucionar isso. Primeiro, crie o diretório novo <code>/pods</code>.</p>
<div class="paragraph">
<p>A seguir, execute o pod <code>writer</code> com as mesmas configurações utilizadas no passo (a), mas desta vez utilizando um volume do tipo <code>HostPath</code> que mapeie o diretório <code>/pods</code> no <em>node</em> para o diretório <code>/data</code> no contexto do pod.</p>
</div>
<div class="paragraph">
<p>Finalmente, acesse um <em>shell</em> interativo no pod e crie o arquivo <code>/data/hello</code> com o conteúdo <code>world</code>, e verifique: o mesmo arquivo existe dentro da pasta <code>/pods</code> no <em>node</em>? Se não, porquê?</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Criar o diretório, evidentemente, é bastante simples.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># mkdir /pods</strong></pre>
</div>
</div>
<div class="paragraph">
<p>A seguir, criamos o pod <code>writer</code>. Desta vez, como queremos especificar um volume persistente, iremos defini-lo através de um arquivo YAML com o conteúdo que se segue:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">writer</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">writer</span>
    <span class="na">args</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">sleep</span>
    <span class="pi">-</span> <span class="s2">"</span><span class="s">3600"</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/data</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">data-dir</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data-dir</span>
    <span class="na">hostPath</span><span class="pi">:</span>
      <span class="na">path</span><span class="pi">:</span> <span class="s">/pods</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Crie o pod a partir do arquivo YAML:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f writer.yaml</strong>
pod/writer created</pre>
</div>
</div>
<div class="paragraph">
<p>Agora, acessamos o pod com um <em>shell</em> interativo, como feito anteriormente.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it writer -- /bin/sh</strong></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>/ # whoami ; hostname</strong>
root
writer</pre>
</div>
</div>
<div class="paragraph">
<p>Note, desta vez, que a pasta <code>/data</code> já foi automaticamente criada pelo sistema&#8201;&#8212;&#8201;afinal, é sob esta pasta que o volume persistente encontra-se montado.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>/ # ls -d /data/</strong>
/data/</pre>
</div>
</div>
<div class="paragraph">
<p>Vamos criar o arquivo:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>/ # echo world > /data/hello</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Feito isso, encerre o <em>shell</em> interativo, voltando ao contexto no <em>host</em> <code>s2-master-1</code>. Verifique:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># whoami ; hostname</strong>
root
s2-master-1</pre>
</div>
</div>
<div class="paragraph">
<p>Finalmente, acesse o arquivo criado dentro do pod:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># cat /pods/hello</strong>
cat: /pods/hello: No such file or directory</pre>
</div>
</div>
<div class="paragraph">
<p>Ué&#8230;&#8203; não funcionou? Iremos investigar, a seguir.</p>
</div>
</div>
</details>
</li>
<li>
<p>Onde terá ido parar o arquivo? Para isso, é importante perceber que volumes do tipo <code>hostPath</code> armazenam arquivos na hierarquia de diretórios do <em>node</em> em que o pod está executando.</p>
<div class="paragraph">
<p>Sabendo disso, responda: em qual <em>node</em> o pod <code>writer</code> está executando?</p>
</div>
<div class="paragraph">
<p>Acesse esse <em>node</em> via SSH e determine se o caminho <code>/pods</code> existe, bem como se o arquivo <code>/pods/hello</code> foi criado e possui o conteúdo esperado.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Como dito no enunciado, arquivos armazenados em volumes do tipo <code>hostPath</code> são gravados na hierarquia de diretórios do <em>node</em> em que o pod está executando. Sabendo disso, fica a pergunta: em que <em>node</em> está executando o pod <code>writer</code>?</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pod writer -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName</strong>
NAME     NODE
writer   s2-node-1</pre>
</div>
</div>
<div class="paragraph">
<p>Ah, isso explica tudo! Acesse o <em>node</em> <code>s2-node-1</code> via SSH, via <code>vagrant ssh</code> ou usando o comando que segue:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># sudo -u vagrant ssh -i /home/vagrant/.ssh/tmpkey vagrant@s2-node-1</strong>

(...)

vagrant@s2-node-1:~$</pre>
</div>
</div>
<div class="paragraph">
<p>Uma vez dentro do <em>host</em> <code>s2-node-1</code>, eleve privilégio para <code>root</code>&#8230;&#8203;</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>vagrant@s2-node-1:~$ sudo -i</strong></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>root@s2-node-1:~# whoami ; hostname</strong>
root
s2-node-1</pre>
</div>
</div>
<div class="paragraph">
<p>E liste o diretório <code>/pods</code>. Note que, agora sim, conseguimos visualizar o arquivo criado através do pod <code>writer</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>root@s2-node-1:~# ls /pods/</strong>
hello</pre>
</div>
</div>
<div class="paragraph">
<p>E o seu conteúdo? Será o mesmo?</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>root@s2-node-1:~# cat /pods/hello</strong>
world</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Encerre a sessão SSH no <em>host</em> <code>s2-node-1</code> antes de prosseguir, garantindo que você está logado na máquina <code>s2-master-1</code>, como o usuário <code>root</code>.</p>
<div class="literalblock">
<div class="content">
<pre><strong># whoami ; hostname</strong>
root
s2-master-1</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_2_volumes_persistentes_e_claims">2) Volumes persistentes e <em>claims</em></h3>
<div class="paragraph">
<p>A solução empregada na atividade (1) possui alguns problemas, que iremos atacar em ordem.</p>
</div>
<div class="paragraph">
<p>O primeiro desses problemas é que o arquivo YAML que define o pod <code>writer</code> possui um grande acoplamento entre a definição dos métodos de provisionamento e consumo do <em>storage</em>.</p>
</div>
<div class="paragraph">
<p>Para solucionar isso o Kubernetes provê objetos do tipo <strong>PersistentVolume</strong> (PVs), que podem ser provisionados manualmente por um administrador ou dinamicamente via StorageClasses. PVs são entidades de armazenamento plugáveis, assim como os volumes que utilizamos na atividade anterior, mas possuem um ciclo de vida independente dos pods que o utilizam.</p>
</div>
<div class="paragraph">
<p>Para utilizar PVs temos objetos do tipo <strong>PersistentVolumeClaim</strong> (PVCs), que consistem em requisições de armazenamento por um usuário. Fazendo um paralelo, assim como pods consomem recursos de <em>nodes</em>, como CPU e memória, PVCs consomem recursos de PVs. PVCs podem requisitar especificidades como tamanho a armazenar e tipos de acesso (<code>ReadWriteOnce</code>, <code>ReadOnlyMany</code>, <code>ReadWriteMany</code> ou <code>ReadWriteOncePod</code>).</p>
</div>
<div class="olist loweralpha">
<ol class="loweralpha">
<li>
<p>Como visto acima, diversos tipos de acesso são suportados por PVs e PVCs: <code>ReadWriteOnce</code>, <code>ReadOnlyMany</code>, <code>ReadWriteMany</code> ou <code>ReadWriteOncePod</code>. Consulte a documentação do Kubernetes e responda: o que significam cada um desses modos? Quais suas abreviações, ao utilizar a linha de comando?</p>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Como documentado em <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes" class="bare">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</a> , temos três modos de acesso possíveis para PVs:</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><code>ReadWriteOnce</code>: neste modo, o volume pode ser montado como leitura-escrita por um único <em>node</em>.</p>
</li>
<li>
<p><code>ReadOnlyMany</code>: neste modo, o volume pode ser montado como somente-leitura por múltiplos <em>nodes</em>.</p>
</li>
<li>
<p><code>ReadWriteMany</code>: neste modo, o volume pode ser montado como leitura-escrita por múltiplos <em>nodes</em>.</p>
</li>
<li>
<p><code>ReadWriteOncePod</code>: neste modo, o volume pode ser montado como leitura-escrita por um único pod. Suportado apenas para volumes CSI e Kubernetes versões 1.22 e acima.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>Note que a definição dos modos acima especifica que os volumes serão montados por <strong><em>nodes</em></strong>, e não por <strong>pods</strong>. Outro aspecto relevante é que um volume só pode ser montado utilizando um desses modos de acesso, mesmo que múltiplos modos sejam suportados.</p>
</div>
<div class="paragraph">
<p>Quanto às abreviações ao utilizar a linha de comando, estas são:</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><code>RWO</code> : <code>ReadWriteOnce</code></p>
</li>
<li>
<p><code>ROX</code> : <code>ReadOnlyMany</code></p>
</li>
<li>
<p><code>RWX</code> : <code>ReadWriteMany</code></p>
</li>
<li>
<p><code>RWOP</code>: <code>ReadWriteOncePod</code></p>
</li>
</ul>
</div>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Vamos começar pelo PersistentVolume. Crie um PV com o nome <code>pv-data</code> e tipo <code>hostPath</code>, requisitando um espaço de 200Mi e modo de acesso <code>ReadWriteMany</code>. Utilize o mesmo caminho do volume especificado na atividade anterior.</p>
<div class="paragraph">
<p>A seguir, verifique o funcionamento de sua configuração com o comando <code>kubectl get pv</code>.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>O arquivo YAML de definição do PV é relativamente simples, como visto abaixo.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pv-data</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">200Mi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/pods"</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Para criá-lo, basta utilizar <code>kubectl apply</code> (ou <code>create</code>).</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f pv-data.yaml</strong>
persistentvolume/pv-data created</pre>
</div>
</div>
<div class="paragraph">
<p>Para visualizar os dados do objeto utilize <code>kubectl get pv</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv pv-data</strong>
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-data   200Mi      RWX            Retain           Available                                   36s</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Agora, para o PersistentVolumeClaim. Crie o PVC <code>pvc-data</code>, com requerimento de armazenamento de 100Mi e modo de acesso <code>ReadWriteOnce</code>.</p>
<div class="paragraph">
<p>A seguir, verifique: quais são os estados do PVC <code>pvc-data</code> e do PV <code>pv-data</code>. Porquê?</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>O arquivo YAML que define o PVC é ainda mais simples que o anterior. Veja:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pvc-data</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">100Mi</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>A seguir, criamos o objeto&#8230;&#8203;</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f pvc-data.yaml</strong>
persistentvolumeclaim/pvc-data created</pre>
</div>
</div>
<div class="paragraph">
<p>E verificamos seu estado. Note que ele se encontra como <code>Pending</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pvc pvc-data</strong>
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-data   Pending                                                     5s</pre>
</div>
</div>
<div class="paragraph">
<p>O PV, por outro lado, ainda é marcado como <code>Available</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv pv-data</strong>
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-data   200Mi      RWX            Retain           Available                                   5m24s</pre>
</div>
</div>
<div class="paragraph">
<p>Ao investigar os eventos do PVC, o motivo fica claro: nenhum PV é identificado como disponível para atender os requisitos do PVC. Mas porquê?</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl describe pvc pvc-data | tail -n1</strong>
  Normal  FailedBinding  9s (x6 over 82s)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Corrija o problema identificado com o PVC <code>pvc-data</code>.</p>
<div class="paragraph">
<p>A seguir, verifique o estado do PVC <code>pvc-data</code> e do PV <code>pv-data</code> para garantir o funcionamento de sua configuração.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>A razão para o erro identificado no passo anterior é que o PV e o PVC não possuem modos de acesso compatíveis: enquanto o PV possui o modo <code>ReadWriteMany</code>, o PVC exige o modo <code>ReadWriteOnce</code>. Para corrigir isso, basta editar o arquivo YAML de definição do PVC:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># sed -i 's/ReadWriteOnce/ReadWriteMany/' pvc-data.yaml</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Agora, delete o objeto e recrie-o.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pvc pvc-data ; kubectl apply -f pvc-data.yaml</strong>
persistentvolumeclaim "pvc-data" deleted
persistentvolumeclaim/pvc-data created</pre>
</div>
</div>
<div class="paragraph">
<p>Ao visualizar o estado do PVC, imediatamente notamos que ele está marcado como <code>Bound</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pvc pvc-data</strong>
NAME       STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-data   Bound    pv-data   200Mi      RWX                           8s</pre>
</div>
</div>
<div class="paragraph">
<p>Semelhantemente, o PV foi atualizado e agora possui o mesmo estado.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv pv-data</strong>
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE
pv-data   200Mi      RWX            Retain           Bound    default/pvc-data                           9m15s</pre>
</div>
</div>
<div class="paragraph">
<p>Note, ainda, que o espaço disponível para o PVC <code>pvc-data</code> é de 200Mi, e não os 100Mi solicitados originalmente. Dado que o PV oferece um espaço maior que o solicitado pelo PVC, o Kubernetes identifica que é possível atender o requisito do <em>claim</em> sem maiores problemas. Se a situação fosse invertida, contudo (isto é, o PVC solicitando um espaço de armazenamento superior ao que é disponibilizado pelo PV), a requisição não poderia ser atendida.</p>
</div>
</div>
</details>
</li>
<li>
<p>Faça com que o pod <code>writer</code> utilize o PVC <code>pvc-data</code> criado no passo anterior, substituindo o volume anteriormente configurado.</p>
<div class="paragraph">
<p>Após a recriação do pod, crie o arquivo novo <code>/data/pvc-hello</code> com o conteúdo <code>world</code>, e valide sua presença no sistema de arquivos do <em>node</em> hospedeiro.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Para fazer a alteração solicitada, basta editar a seção <code>spec.volumes</code> do arquivo YAML. Seu conteúdo deverá ficar assim:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">writer</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">writer</span>
    <span class="na">args</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">sleep</span>
    <span class="pi">-</span> <span class="s2">"</span><span class="s">3600"</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/data</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">data-dir</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data-dir</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">pvc-data</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Remova o pod, e recrie-o usando o novo arquivo de definição.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pod writer ; kubectl apply -f writer.yaml</strong>
pod "writer" deleted
pod/writer created</pre>
</div>
</div>
<div class="paragraph">
<p>Agora, utilize o <code>kubectl exec</code> para criar o arquivo solicitado. Veja que podemos realizar essa ação sem necessariamente iniciar um <em>shell</em> interativo, com o comando que se segue.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it writer -- /bin/sh -c 'echo world > /data/pvc-hello'</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Como estabelecido, o pod <code>writer</code> está executando no <em>node</em> <code>s2-node-1</code> (devido ao <em>node</em> <code>s2-master-1</code> possuir um <em>taint</em> aplicado, como vimos na sessão 3). Para maior agilidade, podemos iniciar uma sessão SSH e verificar o conteúdo do arquivo criado pelo comando anterior em um <em>one-liner</em>. Veja:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># sudo -u vagrant ssh -i /home/vagrant/.ssh/tmpkey vagrant@s2-node-1 cat /pods/pvc-hello</strong>
world</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Considere a política de recuperação (<em>Reclaim Policy</em>) do PV <code>pv-data</code>. O que ocorreria com esse PV caso o PVC <code>pvc-data</code> fosse removido?</p>
<div class="paragraph">
<p>Quais outras políticas de recuperação poderiam ter sido escolhidas?</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>A política de recuperação do PV pode ser visualizada com os comandos <code>kubectl get</code> ou <code>kubectl describe</code>. Indo direto ao ponto, podemos usar JSONPath para buscar apenas o campo relevante:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv pv-data -o custom-columns=NAME:.metadata.name,RECLAIMPOL:.spec.persistentVolumeReclaimPolicy</strong>
NAME      RECLAIMPOL
pv-data   Retain</pre>
</div>
</div>
<div class="paragraph">
<p>Quanto aos demais <em>reclaim policies</em> possíveis, estes estão documentados em <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming" class="bare">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming</a> . Vejamos quais são eles:</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><strong><em>Retain</em></strong>: este modo permite a recuperação manual do recurso. Quanto o PVC é removido, o PV permanece inalterado e o volume é marcado como "liberado". Ele ainda não pode ser utilizado por outro <em>claim</em>, contudo, já que os dados do <em>claim</em> anterior ainda não foram tratados.</p>
</li>
<li>
<p><strong><em>Delete</em></strong>: neste modo, a deleção do PVC ocasiona a remoção também do PV e do <em>asset</em> de armazenamento em infraestrutura externa suportada, como AWS EBS, GCE PD ou volume Cinder.</p>
</li>
<li>
<p><strong><em>Recycle</em></strong>: neste modo, o sistema realiza uma limpeza básica (<code>rm -rf /${VOLUME}/*</code>) e o marca como disponível para outro <em>claim</em>.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Vamos testar! Remova o PVC <code>pvc-data</code> e verifique: o que acontece? Porquê?</p>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Vamos lá:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pvc pvc-data</strong>
persistentvolumeclaim "pvc-data" deleted
&lt;HANGUP&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Após esse comando, o PVC fica travado em estado <code>Terminating</code>, e o <em>shell</em> fica indisponível. Isto ocorre porque o PVC ainda está sendo utilizado por um pod ativo (no caso, o pod <code>writer</code>).</p>
</div>
</div>
</details>
</li>
<li>
<p>Encerre o processo de remoção do PVC e, a seguir, remova o pod <code>writer</code>. O que acontece com o PVC <code>pvc-data</code> e o PV <code>pv-data</code>?</p>
<div class="paragraph">
<p>Verifique o conteúdo do diretório <code>/pods</code> no <em>node</em> hospedeiro: o que aconteceu com os dados armazenados nesse local?</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Para retomar o controle do <em>shell</em>, digite <code>CTRL + C</code>. Em seguida, remova o pod:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pod writer</strong>
pod "writer" deleted</pre>
</div>
</div>
<div class="paragraph">
<p>Note que o PVC é removido imediatamente:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pvc pvc-data</strong>
Error from server (NotFound): persistentvolumeclaims "pvc-data" not found</pre>
</div>
</div>
<div class="paragraph">
<p>O PV, por outro lado, fica com o estado <em>Released</em> (liberado)&#8201;&#8212;&#8201;como seria de se esperar para o <em>access mode</em> <code>Retain</code>, explicado anteriormente.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv pv-data</strong>
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM              STORAGECLASS   REASON   AGE
pv-data   200Mi      RWX            Retain           Released   default/pvc-data                           23m</pre>
</div>
</div>
<div class="paragraph">
<p>De fato, os dados escritos pelo pod <code>writer</code> ainda permanecem acessíveis no volume persistente, como constatado pelo comando abaixo:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># sudo -u vagrant ssh -i /home/vagrant/.ssh/tmpkey vagrant@s2-node-1 cat /pods/pvc-hello</strong>
world</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Antes de prosseguir, remova o PV <code>pv-data</code>.</p>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pv pv-data</strong>
persistentvolume "pv-data" deleted</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_3_storage_classes">3) <em>Storage Classes</em></h3>
<div class="paragraph">
<p>StorageClasses são formas dos administradores de um <em>cluster</em> Kubernetes descreverem as classes que armazenamento oferecidas pelo ambiente. Essas classes podem mapear níveis de qualidade de serviço, políticas de backup, velocidade de acesso ou quaisquer outros atributos arbitrários definidos pelos administradores.</p>
</div>
<div class="paragraph">
<p>Uma funcionalidade interessante provida por StorageClasses é o fato de que podemos provisionar volumes de forma dinâmica, isto é, sob demanda. Sem esse tipo de recurso os administradores devem, primeiro, providenciar volumes manualmente em seus ambiente de <em>storage</em> ou no provedor de <em>cloud</em> e, segundo, criar objetos do tipo PersistentVolume  para representá-los no Kubernetes. Com o provisionamento dinâmico, esses recursos são criados quando solicitados pelo usuário.</p>
</div>
</div>
<div class="sect2">
<h3 id="_3_1_provisionamento_manual">3.1) Provisionamento manual</h3>
<div class="paragraph">
<p>Para melhor ambientação com StorageClasses iremos primeiramente trabalhar com o <em>provisioner</em> <code>Local</code>, que não suporta provisionamento dinâmico. Não obstante, ainda neste caso é vantajoso trabalhar com StorageClasses em lugar de PVs diretamente, por motivos que veremos nos passo a seguir.</p>
</div>
<div class="olist loweralpha">
<ol class="loweralpha">
<li>
<p>Antes de criar o StorageClass propriamente dito, é importante saber que além do seu <em>provisioner</em> (o <em>plugin</em> utilizado para provisionar PVs, como NFS, CephFS, Glusterfs ou AWSElasticBlockStore), um outro parâmetro a ser definido é o modo de associação (<em>Volume Binding Mode</em>) a ser utilizado.</p>
<div class="paragraph">
<p>Como estabelecido anteriormente, nesta atividade iremos utilizar o StorageClass com um <em>provisioner</em> do tipo <code>Local</code>. Pesquise na documentação oficial do Kubernetes, e responda: quais <em>Volume Binding Modes</em> são suportados por esse <em>provisioner</em>? O que cada um deles faz?</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Como documentado em <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode" class="bare">https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode</a> , há dois <em>Volume Binding Modes</em> disponíveis: <code>Immediate</code> e <code>WaitForFirstConsumer</code>:</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><code>Immediate</code>: por padrão, este modo indica que o <em>binding</em> do volume ocorrerá assim que o PVC for criado. Assim, PVs serão associados ou provisionados sem conhecimento dos requerimentos de agendamento de pods. Para <em>backends</em> de armazenamento restritos por topologia ou inacessíveis a todos os <em>nodes</em> do <em>cluster</em>, isso pode resultar em pods não-agendáveis.</p>
</li>
<li>
<p><code>WaitForFirstConsumer</code>: a situação acima pode ser solucionada com o modo <code>WaitForFirstConsumer</code>, que atrasa a associação e criação de PVs até que um pod que utilize o PVC seja criado. PVs serão, então, selecionados ou provisionados considerando a topologia especificada pelos requisitos de agendamento do pod, incluindo aspectos como requerimento de recursos, seletores de <em>nodes</em>, afinidade e anti-afinidade de pods e <em>taints</em> e <em>tolerations</em>.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Agora que os <em>Volume Binding Modes</em> válidos para o <em>provisioner</em> <code>Local</code> são conhecidos, vamos ao trabalho.</p>
<div class="paragraph">
<p>Crie um StorageClass com o nome <code>sc-data</code>, <em>provisoner</em> do tipo <code>Local</code> e <em>Volume Binding Mode</em> <code>WaitForFirstConsumer</code>. Verifique o funcionamento de sua configuração.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>O arquivo YAMl que define o StorageClass é bastante simples, e mostrado a seguir.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">sc-data</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">kubernetes.io/no-provisioner</span>
<span class="na">volumeBindingMode</span><span class="pi">:</span> <span class="s">WaitForFirstConsumer</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Crie-o com <code>kubectl apply</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f sc-data.yaml</strong>
storageclass.storage.k8s.io/sc-data created</pre>
</div>
</div>
<div class="paragraph">
<p>E, finalmente, verifique o sucesso na criação do objeto.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get sc</strong>
NAME      PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
sc-data   kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false                  3s</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Qual é a política de recuperação (<em>Reclaim Policy</em>) do StorageClass <code>sc-data</code>? Porquê?</p>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Esse dado é mostrado no último comando do passo acima. Podemos também buscar o campo específico:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get sc sc-data -o custom-columns=NAME:.metadata.name,RECLAIMPOL:.reclaimPolicy</strong>
NAME      RECLAIMPOL
sc-data   Delete</pre>
</div>
</div>
<div class="paragraph">
<p>Como documentado em <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy" class="bare">https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy</a> , PVs dinamicamente criados por um StorageClass terão sua política de recuperação atribuída por herança. Caso nenhuma política seja especificada quando da criação do StorageClass (exatamente o que fizemos, no passo anterior), o <em>reclaim policy</em> assumirá o valor padrão <code>Delete</code>.</p>
</div>
</div>
</details>
</li>
<li>
<p>Como visto anteriormente o StorageClass do tipo <code>Local</code> não suporta provisionamento dinâmico&#8201;&#8212;&#8201;assim sendo, iremos fazê-lo manualmente.</p>
<div class="paragraph">
<p>Crie o PV <code>sc-pv-data</code> com tamanho de 250 Mi, modo de acesso <code>ReadWriteOnce</code>, política de recuperação <code>Retain</code> e o mesmo caminho usado anteriormente, <code>/pods</code>. Evidentemente, faça com que esse PV utilize o StorageClass <code>sc-data</code>.</p>
</div>
<div class="paragraph">
<p>Mais um detalhe: algo que foi omitido na atividade anterior, mas que é bastante relevante, é que PVs do tipo <code>local</code> devem explicitamente ajustar a afinidade em relação a <em>nodes</em>&#8201;&#8212;&#8201;assim, pods que utilizem esses PVs serão agendados apenas em <em>nodes</em> que podem ser selecionados. Essa características não é tão relevante no momento, já que nosso <em>cluster</em> possui apenas um <em>node</em> agendável (a máquina <code>s2-node-1</code>), mas deve ser considerado em <em>cluster</em> maiores. Essa característica é documentada neste link: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#node-affinity" class="bare">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#node-affinity</a></p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>O arquivo YAML que define o PV é mostrado abaixo. Note o uso do atributo <code>spec.nodeAffinity</code> para garantir o <em>node</em> de agendamento do PV, bem como a customização do <em>reclaim policy</em> via <code>spec.persistentVolumeReclaimPolicy</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">sc-pv-data</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">250Mi</span>
  <span class="na">local</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/pods</span>
  <span class="na">nodeAffinity</span><span class="pi">:</span>
    <span class="na">required</span><span class="pi">:</span>
      <span class="na">nodeSelectorTerms</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">matchExpressions</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">kubernetes.io/hostname</span>
          <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
          <span class="na">values</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">s2-node-1</span>
  <span class="na">persistentVolumeReclaimPolicy</span><span class="pi">:</span> <span class="s">Retain</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">sc-data</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Crie o objeto:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f sc-pv-data.yaml</strong>
persistentvolume/sc-pv-data created</pre>
</div>
</div>
<div class="paragraph">
<p>E verifique o funcionamento de sua configuração.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv</strong>
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
sc-pv-data   250Mi      RWO            Retain           Available           sc-data                 4s</pre>
</div>
</div>
<div class="paragraph">
<p>Note que mesmo com o <em>reclaim policy</em> do StorageClass <code>sc-data</code> sendo igual a <code>Delete</code>, conseguimos suplantá-lo com uma configuração específica no arquivo YAML que define o PV manualmente provisionado, como objetivado.</p>
</div>
</div>
</details>
</li>
<li>
<p>Perfeito! Agora, crie o PVC <code>sc-pvc-data</code>; utilize as mesmas características (<code>accessMode</code> e tamanho de armazenamento) configurados para o PV <code>sc-pv-data</code>. Não se esqueça de mencionar o uso do StorageClass apropriado.</p>
<div class="paragraph">
<p>Qual é o estado desse PVC após sua criação? Porquê?</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Vamos lá:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">sc-pvc-data</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">sc-data</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">250Mi</span></code></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f sc-pvc-data.yaml</strong>
persistentvolumeclaim/sc-pvc-data created</pre>
</div>
</div>
<div class="paragraph">
<p>Note que o estado do PVC, após sua criação, é marcado como <code>Pending</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pvc sc-pvc-data</strong>
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
sc-pvc-data   Pending                                      sc-data        30s</pre>
</div>
</div>
<div class="paragraph">
<p>Isto se deve ao uso do <em>volume binding mode</em> <code>WaitForFirstConsumer</code>, atribuído ao StorageClass. De fato, ao observarmos os eventos do PVC, é exatamente este o motivo pelo qual seu estado é marcado como pendente: ele está aguardando a criação do primeiro consumidor antes de associar-se a um PV.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl describe pvc sc-pvc-data | tail -n1</strong>
  Normal  WaitForFirstConsumer  12s (x15 over 3m30s)  persistentvolume-controller  waiting for first consumer to be created before binding</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Finalmente, recrie o pod <code>writer</code>, desta vez utilizando o PVC <code>sc-pvc-data</code> criado no passo anterior.</p>
<div class="paragraph">
<p>Após a criação do pod, crie o arquivo novo <code>/data/sc-pvc-hello</code> com o conteúdo <code>world</code>, e valide sua presença no sistema de arquivos do <em>node</em> hospedeiro.</p>
</div>
<div class="paragraph">
<p>Finalmente, verifique: qual o estado do PV <code>sc-pv-data</code>? E quanto ao PVC <code>sc-pvc-data</code>?</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Para criar o pod segundo as especificações, basta editar o nome do PVC utilizado via <code>sed</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># sed -i 's/pvc-data/sc-pvc-data/' writer.yaml</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Em seguida, criamos o pod&#8230;&#8203;</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f writer.yaml</strong>
pod/writer created</pre>
</div>
</div>
<div class="paragraph">
<p>E o arquivo solicitado:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it writer -- /bin/sh -c 'echo world > /data/sc-pvc-hello'</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Verifique que o arquivo foi, de fato, escrito no volume persistente no <em>node</em> <code>s2-node-1</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># sudo -u vagrant ssh -i /home/vagrant/.ssh/tmpkey vagrant@s2-node-1 cat /pods/sc-pvc-hello</strong>
world</pre>
</div>
</div>
<div class="paragraph">
<p>Uma vez criado o pod, note que o PV é marcado como <code>Bound</code>, assim como o PVC. Este é o comportamento esperado ao utilizar o <em>volume binding mode</em> <code>WaitForFirstConsumer</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv sc-pv-data</strong>
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
sc-pv-data   250Mi      RWO            Retain           Bound    default/sc-pvc-data   sc-data                 11m</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pvc</strong>
NAME          STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
sc-pvc-data   Bound    pv-data   250Mi      RWO            sc-data        7m55s</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Antes de partir para a próxima atividade, remova todos os objetos criados até aqui. Iremos arquitetar uma maneira melhor de lidar com a persistência de dados, a seguir.</p>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pod writer ; kubectl delete pvc sc-pvc-data ; kubectl delete pv sc-pv-data ; kubectl delete sc sc-data</strong>
pod "writer" deleted
persistentvolumeclaim "sc-pvc-data" deleted
persistentvolume "sc-pv-data" deleted
storageclass.storage.k8s.io "sc-data" deleted</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_3_2_provisionamento_dinâmico">3.2) Provisionamento dinâmico</h3>
<div class="paragraph">
<p>Muito embora a configuração realizada na atividade anterior tenha funcionado, ela ainda não é ideal.</p>
</div>
<div class="paragraph">
<p>Imagine, por exemplo, que o pod <code>writer</code> fosse agendado em outro <em>node</em>: o que ocorreria nesse caso? Como o PV foi configurado com afinidade específica para o <em>node</em> <code>s2-node-1</code>, o <em>cluster</em> teria problemas em agendar o pod.</p>
</div>
<div class="paragraph">
<p>Considere, ainda, um serviço em que múltiplos pods precisam ler o mesmo dado&#8201;&#8212;&#8201;como uma pasta compartilhada ou um <em>cluster</em> de banco de dados. Ora, como o armazenamento configurado até aqui foi totalmente local, é fácil imaginar que dados escritos em um dos <em>nodes</em> não seria propagado para os demais.</p>
</div>
<div class="paragraph">
<p>Finalmente, note que tivemos que criar manualmente o PV antes que pudéssemos associá-lo a um PVC, e então a um pod. Essa configuração é bastante envolvida, e nada prática.</p>
</div>
<div class="paragraph">
<p>Para resolver essas questões, iremos implementar nesta atividade o provisionamento dinâmico de PVs. Para garantir que os dados fiquem sincronizados entre os diferentes <em>nodes</em> do <em>cluster</em>, utilizaremos uma solução de armazenamento distruído simples&#8201;&#8212;&#8201;neste caso o <em>Network File System</em> (NFS). Vamos lá?</p>
</div>
<div class="olist loweralpha">
<ol class="loweralpha">
<li>
<p>Antes de mais nada, temos que implementar o servidor NFS&#8201;&#8212;&#8201;felizmente, isso é bastante simples. Iremos criar um servidor NFS na máquina <code>s2-master-1</code>, servindo o diretório <code>/pods</code>.</p>
<div class="paragraph">
<p>Note que esse diretório foi criado anteriormente, mas nunca foi de fato utilizado porque todos os pods foram agendados no <em>host</em> <code>s2-node-1</code> até aqui:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># whoami ; hostname</strong>
root
s2-master-1</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># ls -1a /pods</strong>
.
..</pre>
</div>
</div>
<div class="paragraph">
<p>Instale os pacotes necessário ao funcionamento do NFS do lado do servidor:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># apt install -y nfs-kernel-server nfs-common portmap</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Iremos, a seguir, realizar uma configuração simples (porém insegura) de um servidor NFS para testar o provisionamento dinâmico de volumes.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Aviso"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Como mencionado, a configuração utilizada aqui foi propositalmente simplificada para acelerar a implantação do serviço e focar no tema-alvo da sessão, que é a gestão de armazenamento no Kubernetes.</p>
</div>
<div class="paragraph">
<p>A configuração detalhada de permissionamento, tanto no nível do SO, serviço e ambiente de orquestração de containers aumentaria significativamente a complexidade de vários elementos, incluindo os arquivos YAML de definição de objetos, tornando a atividade consideralmente mais complexa. Nesse caso, a atividade provavelmente teria que ser realizada de forma guiada (isto é, fornecendo ao aluno as perguntas e também respostas, juntamente com comandos), alterando a didática utilizada até aqui.</p>
</div>
<div class="paragraph">
<p>Em um ambiente de produção, é crítico que o permissionamento de diretórios e controle de quais usuários podem ler e escrever arquivos seja cuidadosamente planejado. Assim, ao realizar este tipo de implantação em sua organização, tenha atenção ao fazer os ajustes necessários.</p>
</div>
<div class="paragraph">
<p>Como um desafio, sugere-se que o aluno tente adaptar este roteiro para controlar apropriadamente as permissões de leitura e escrita para os diferentes pods e seus usuários/grupos efetivos. Todos os conhecimentos necessários para este fim já foram tratados até aqui, neste curso. Você está preparado?</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Vamos lá. Popule o arquivo <code>/etc/exports</code> com o diretório que será exportado pelo servidor NFS:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># echo '/pods *(rw,sync,no_subtree_check,no_root_squash,no_all_squash,insecure)' >> /etc/exports</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Atualize a lista de <em>exports</em> do servidor:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># exportfs -rv</strong>
exporting *:/pods</pre>
</div>
</div>
<div class="paragraph">
<p>E verifique seu funcionamento:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># showmount -e</strong>
Export list for s2-master-1:
/pods *</pre>
</div>
</div>
<div class="paragraph">
<p>Perfeito. Vamos agora verificar o funcionamento de nossa configuração: acesse o <em>host</em> <code>s2-node-1</code> e veja se ele consegue montar e visualizar o diretório compartilhado via NFS. Comece efetuando login no <em>host</em>, como o usuário <code>root</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># sudo -u vagrant ssh -i /home/vagrant/.ssh/tmpkey vagrant@s2-node-1</strong>

(...)

vagrant@s2-node-1:~$</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>vagrant@s2-node-1:~$ sudo -i</strong></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>root@s2-node-1:~# hostname ; whoami</strong>
s2-node-1
root</pre>
</div>
</div>
<div class="paragraph">
<p>Teste a montagem do diretório remoto em qualquer local, por exemplo, na pasta <code>/mnt</code>. Note que utilizaremos o endereço IP do <em>host</em> <code>s2-master-1</code>, e não seu nome de domínio&#8201;&#8212;&#8201;isto se deve ao fato de que este <em>hostname</em> é definido através do arquivo <code>/etc/hosts</code> no <em>node</em> <code>s2-node-1</code> apenas, e não em um servidor DNS, e é portanto inválido dentro do contexto de pods criados dentro do <em>cluster</em>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>root@s2-node-1:~# mount -t nfs 192.168.68.20:/pods /mnt</strong></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>root@s2-node-1:~# mount | grep '^192.168.68.20:/pods'</strong>
192.168.68.20:/pods on /mnt type nfs4 (rw,relatime,vers=4.2,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.68.25,local_lock=none,addr=192.168.68.20)</pre>
</div>
</div>
<div class="paragraph">
<p>Tudo certo! Antes de prosseguir, desmonte o diretório.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong>root@s2-node-1:~# umount /mnt</strong></pre>
</div>
</div>
</li>
<li>
<p>Vamos agora realizar a implantação do <em>NFS-Client Provisioner</em> (<a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner" class="bare">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner</a>), uma solução criada para suportar o provisionamento dinâmico de PVs no Kubernetes em um servidor NFS preexistente&#8201;&#8212;&#8201;como o que criamos no passo anterior.</p>
<div class="paragraph">
<p>Primeiro, garanta que você está na máquina <code>s2-master-1</code>, como usuário <code>root</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># hostname ; whoami</strong>
s2-master-1
root</pre>
</div>
</div>
<div class="paragraph">
<p>Vamos agora criar um ServiceAccount para o serviço de provisionamento, juntamente com um ClusterRole, ClusterRoleBinding, Role e RoleBinding. Observe que o arquivo utilizado no comando a seguir configura o provisionador para funcionamento no namespace <em>default</em>&#8201;&#8212;&#8201;para alterar isso, basta editar o arquivo YAML antes de aplicá-lo ao <em>cluster</em>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/rbac.yaml</strong>
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created</pre>
</div>
</div>
<div class="paragraph">
<p>Vamos verificar o funcionamento do comando:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get clusterrole,clusterrolebinding,role,rolebinding | grep nfs | cut -d' ' -f1</strong>
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner</pre>
</div>
</div>
<div class="paragraph">
<p>A seguir criaremos o StorageClass que irá consumir o recurso de armazenamento NFS. Usaremos como base o arquivo YAML disponível no mesmo GitHub do projeto <em>NFS-Client Provisioner</em>, apenas editando o nome do <em>provisioner</em> para um nome mais em linha com o recurso que configuramos no passo (a).</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># curl -s https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/class.yaml | \
  sed 's/^\(provisioner:\).&#42;/\1 nfs.contorq.com/' | \
  kubectl apply -f -</strong>
storageclass.storage.k8s.io/nfs-client created</pre>
</div>
</div>
<div class="paragraph">
<p>Verifique o sucesso da configuração:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get sc nfs-client</strong>
NAME         PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-client   nfs.contorq.com   Delete          Immediate           false                  66s</pre>
</div>
</div>
<div class="paragraph">
<p>Finalmente, iremos criar o deployment: este será o serviço que efetivamente irá monitorar por novos PVCs e criar PVs correspondentes automaticamente, alocando-os dentro do servidor NFS. Novamente, teremos que adaptar o nome do <em>provisioner</em> (assim como feito na criação do StorageClass, acima), bem como o endereço IP do servidor NFS e o caminho exportado por ele.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># curl -s https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/deployment.yaml | \
  sed '/PROVISIONER_NAME/{n;s/^\([[:space:]]&#42;value:\).&#42;/\1 nfs.contorq.com/;}' | \
  sed '/NFS_SERVER/{n;s/^\([[:space:]]&#42;value:\).&#42;/\1 192.168.68.20/;}' | \
  sed 's/^\([[:space:]]&#42;server:\).&#42;/\1 192.168.68.20/' | \
  sed '/NFS_PATH/{n;s/^\([[:space:]]&#42;value:\).&#42;/\1 \/pods/;}' | \
  sed 's/^\([[:space:]]&#42;path:\).&#42;/\1 \/pods/' | \
  kubectl apply -f -</strong>
deployment.apps/nfs-client-provisioner created</pre>
</div>
</div>
<div class="paragraph">
<p>A seguir, verifique se o deployment, e seu pod correspondente, foram criados.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get deploy,pod</strong>
NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nfs-client-provisioner   1/1     1            1           29s

NAME                                          READY   STATUS    RESTARTS   AGE
pod/nfs-client-provisioner-85f8f98c95-fs8pv   1/1     Running   0          29s</pre>
</div>
</div>
</li>
<li>
<p>Excelente! Hora de testar o ambiente: primeiro, verifique que não existe nenhum PersistentVolume ou PersistentVolumeClaim no ambiente.</p>
<div class="paragraph">
<p>A seguir, crie o PVC <code>dynamic-pvc-data</code> que utilize o StorageClass criado no passo anterior, usando o <code>accessMode</code> <code>ReadWriteMany</code> e tamanho de 500Mi.</p>
</div>
<div class="paragraph">
<p>Finalmente, cheque se o PVC foi criado, bem como um PV correspondente.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Vamos verificar a existência de PVs e PVCs remanescentes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv,pvc</strong>
No resources found</pre>
</div>
</div>
<div class="paragraph">
<p>Agora, para o PVC: utilize o arquivo YAML que se segue.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
</pre></td><td class="code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">dynamic-pvc-data</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">nfs-client</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">500Mi</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>Crie o objeto com <code>kubectl apply</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f dynamic-pvc-data.yaml</strong>
persistentvolumeclaim/dynamic-pvc-data created</pre>
</div>
</div>
<div class="paragraph">
<p>A seguir, verifique o estado do PVC e seu PV correspondente. Note que o estado do PVC criado é <em>Bound</em>&#8201;&#8212;&#8201;isto se deve ao <em>volume binding mode</em> utilizado no StorageClass criado no passo anterior, que é <code>Immediate</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pvc dynamic-pvc-data</strong>
NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
dynamic-pvc-data   Bound    pvc-13a54631-3798-41ba-909d-24ae94e3f262   500Mi      RWX            nfs-client   48s</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pv</strong>
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS          REASON   AGE
pvc-13a54631-3798-41ba-909d-24ae94e3f262   500Mi      RWX            Delete           Bound    default/dynamic-pvc-data   nfs-client            61s</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Agora, recrie o pod <code>writer</code>, desta vez instruindo-o a utilizar o PVC <code>dynamic-pvc-data</code> criado no passo anterior.</p>
<div class="paragraph">
<p>Após a criação do pod, crie o arquivo novo <code>/data/dynamic-pvc-hello</code> com o conteúdo <code>world</code>. Em seguida, responda: onde é garantida a persistência desse arquivo? Aponte o local onde o arquivo foi criado, no servidor NFS.</p>
</div>
<div class="paragraph">
<p>Outra pergunta: em qual <em>node</em> foi agendado o pod <code>writer</code>? Em que aspecto isso difere da configuração realizada na atividade anterior.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Novamente, para alterar a configuração do pod <code>writer</code> basta utilizar o comando <code>sed</code> e editar o nome do PVC em uso.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># sed -i 's/sc-pvc-data/dynamic-pvc-data/' writer.yaml</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Crie o pod e escreva o arquivo solicitado.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f writer.yaml</strong>
pod/writer created</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it writer -- /bin/sh -c 'echo world > /data/dynamic-pvc-hello'</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Dentro da pasta compartilhada pelo servidor NFS, <code>/pods</code>, note que um diretório foi criado automaticamente:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># ls /pods</strong>
default-dynamic-pvc-data-pvc-13a54631-3798-41ba-909d-24ae94e3f262</pre>
</div>
</div>
<div class="paragraph">
<p>Dentro dele está o arquivo criado pelo pod <code>writer</code>, com o conteúdo que se espera.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># cat /pods/default-dynamic-pvc-data-pvc-13a54631-3798-41ba-909d-24ae94e3f262/dynamic-pvc-hello</strong>
world</pre>
</div>
</div>
<div class="paragraph">
<p>Veja que o pod <code>writer</code> está de fato sendo executado pelo <em>node</em> <code>s2-node-1</code>, mas seus dados persistentes estão sendo armazenados no servidor NFS <code>s2-master-1</code>. Isto comprova que nossa configuração funcionou, e que agora os locais de execução do pod e armazenamento de seus dados são independentes.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pod writer -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName</strong>
NAME     NODE
writer   s2-node-1</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Finalmente, remova o pod <code>writer</code>. O que acontece com o PVC <code>dynamic-pvc-data</code>? E quanto ao seu PV correspondente?</p>
<div class="paragraph">
<p>A seguir, remova o PVC <code>dynamic-pvc-data</code>. O que acontece com o PV associado? E quanto aos dados armazenados no servidor NFS?</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Vamos remover o pod:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pod writer</strong>
pod "writer" deleted</pre>
</div>
</div>
<div class="paragraph">
<p>Note que o PVC e o PV permanecem no sistema, bem como os dados criados pelo pod.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pvc,pv</strong>
NAME                                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
persistentvolumeclaim/dynamic-pvc-data   Bound    pvc-13a54631-3798-41ba-909d-24ae94e3f262   500Mi      RWX            nfs-client   4m27s

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS          REASON   AGE
persistentvolume/pvc-13a54631-3798-41ba-909d-24ae94e3f262   500Mi      RWX            Delete           Bound    default/dynamic-pvc-data   nfs-client            4m27s</pre>
</div>
</div>
<div class="paragraph">
<p>Agora, remova o PVC:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete pvc dynamic-pvc-data</strong>
persistentvolumeclaim "dynamic-pvc-data" deleted</pre>
</div>
</div>
<div class="paragraph">
<p>Como o <em>Reclaim Policy</em> está configurado como <code>Delete</code>, o PV também é removido no processo.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pvc,pv</strong>
No resources found</pre>
</div>
</div>
<div class="paragraph">
<p>De igual forma, os dados armazenados no servidor NFS também são removidos, quando da exclusão do PVC <code>dynamic-pvc-data</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># ls /pods</strong></pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>As atividades anteriores resolveram dois dos problemas apontados no motivador desta atividade:</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>Agora, não é mais necessário criar manualmente um PV&#8201;&#8212;&#8201;o provisionamento dinâmico do StorageClass <code>nfs-client</code> faz com que PVs sejam criados automaticamente assim que um PVC é solicitado.</p>
</li>
<li>
<p>Adicionalmente, verificamos que mesmo que o pod e o dado não residam no mesmo <em>node</em>, seu agendamento pod e acesso aos dados persistentes é realizado com sucesso.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>Resta, portanto, validar apenas uma questão: será que a solução implementada funciona caso dois pods diferentes precisem ler/escrever no mesmo volume persistente? O atendimento desse requisito é frequentemente necessário, especialmente em aplicações distribuídas como <em>clusters</em> de bancos de dados, por exemplo.</p>
</div>
<div class="paragraph">
<p>Vamos verificar! Crie o PVC <code>novel</code> usando o StorageClass <code>nfs-client</code>, com tamanho de 200Mi.</p>
</div>
<div class="paragraph">
<p>A seguir, crie o deployment <code>author</code>, com 2 réplicas e usando a imagem <code>busybox</code>, executando o comando <code>sleep 3600</code>. Utilize a estratégia de deployment <code>Recreate</code>. Usando anti-afinidade de pods e <em>tolerations</em>, garanta que os pods desse deployment executem em <em>nodes</em> diferentes do <em>cluster</em>. Finalmente, faça com que os pods desse deployment montem um volume persistente usando o PVC <code>novel</code> no diretório <code>/story</code>.</p>
</div>
<div class="paragraph">
<p>Usando um dos pods do deployment <code>author</code>, crie um arquivo novo com qualquer conteúdo dentro do diretório <code>/story</code>; a seguir, acesse a outra réplica do deployment e verifique que seu conteúdo está acessível sob o mesmo diretório.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>Uma vez que as réplicas do deployment serão executadas em <em>nodes</em> distintos, queremos que o PV provisionado dinamicamente pelo PVC <code>novel</code> seja acessado por todos <em>nodes</em> ao mesmo tempo. Como visto anteriormente, o modo que permite leitura-escrita simultânea nesse caso é <code>ReadWriteMany</code>.</p>
</div>
<div class="paragraph">
<p>Sabendo disso, crie o PVC com o arquivo YAML que se segue.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
</pre></td><td class="code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">novel</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">nfs-client</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">200Mi</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f novel.yaml</strong>
persistentvolumeclaim/novel created</pre>
</div>
</div>
<div class="paragraph">
<p>Agora, para o deployment. Note o uso de <em>tolerations</em> e anti-afinidade de pods no arquivo YAML abaixo para atingir os objetivos especificados pelo enunciado, bem como a customização da estratégia do deployment.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
</pre></td><td class="code"><pre><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">author</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">author</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">2</span>
  <span class="na">strategy</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">Recreate</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">author</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">tolerations</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">node-role.kubernetes.io/control-plane"</span>
        <span class="na">operator</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Exists"</span>
        <span class="na">effect</span><span class="pi">:</span> <span class="s2">"</span><span class="s">NoSchedule"</span>
      <span class="na">affinity</span><span class="pi">:</span>
        <span class="na">podAntiAffinity</span><span class="pi">:</span>
          <span class="na">requiredDuringSchedulingIgnoredDuringExecution</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">labelSelector</span><span class="pi">:</span>
              <span class="na">matchExpressions</span><span class="pi">:</span>
              <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">app</span>
                <span class="na">operator</span><span class="pi">:</span> <span class="s">In</span>
                <span class="na">values</span><span class="pi">:</span>
                <span class="pi">-</span> <span class="s">author</span>
            <span class="na">topologyKey</span><span class="pi">:</span> <span class="s2">"</span><span class="s">kubernetes.io/hostname"</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">author</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">busybox</span>
        <span class="na">args</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">sleep</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">3600"</span>
        <span class="na">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/story</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">story-dir</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">story-dir</span>
        <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
          <span class="na">claimName</span><span class="pi">:</span> <span class="s">novel</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl apply -f author.yaml</strong>
deployment.apps/author created</pre>
</div>
</div>
<div class="paragraph">
<p>Verifique o estado do deployment, constatando que os pods estão operacionais e executando em <em>nodes</em> distintos.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get deploy author</strong>
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
author   2/2     2            2           103s</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pod -l app=author -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName</strong>
NAME                      NODE
author-7f85d48844-6qcd6   s2-master-1
author-7f85d48844-998vx   s2-node-1</pre>
</div>
</div>
<div class="paragraph">
<p>Agora criaremos o arquivo <code>/story/chapter1</code> com o conteúdo <code>Once upon a time&#8230;&#8203;</code> em um dos pods acima:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it author-7f85d48844-6qcd6 -- /bin/sh -c 'echo "Once upon a time..." > /story/chapter1'</strong></pre>
</div>
</div>
<div class="paragraph">
<p>No outro pod, listamos o diretório, constatando que o arquivo de fato encontra-se acessível&#8230;&#8203;</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it author-7f85d48844-998vx -- /bin/sh -c 'ls /story'</strong>
chapter1</pre>
</div>
</div>
<div class="paragraph">
<p>E que seu conteúdo é o mesmo que foi inserido originalmente.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it author-7f85d48844-998vx -- /bin/sh -c 'cat /story/chapter1'</strong>
Once upon a time...</pre>
</div>
</div>
<div class="paragraph">
<p>Listando a pasta-raiz do compartilhamento NFS, fica claro que um diretório foi criado para amazenar os dados persistentes, e dentro dele encontra-se o arquivo criado pelo pod.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># ls /pods/</strong>
default-novel-pvc-49b725f6-9d5a-4d5a-927a-2e8044e232c3</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># cat /pods/default-novel-pvc-49b725f6-9d5a-4d5a-927a-2e8044e232c3/chapter1</strong>
Once upon a time...</pre>
</div>
</div>
</div>
</details>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_3_3_múltiplas_utilizações_concorrentes_de_volumes">3.3) Múltiplas utilizações concorrentes de volumes</h3>
<div class="paragraph">
<p>Agora, para um cenário de uso mais complexo. Imagine que vários pods precisam acessar o mesmo PV, como vimos no último passo da atividade anterior, mas que os dados desses pods devem ser individualizados.</p>
</div>
<div class="paragraph">
<p>Você pode estar se perguntando: em que cenário isso seria necessário?</p>
</div>
<div class="paragraph">
<p>Simples! Suponha, por exemplo, que queremos apontar um analisador de registros de log para um diretório em que diversos pods escrevem seus eventos. Ora, é bastante provável que esses pods escrevam seus registros em arquivos com o mesmo nome (digamos, <code>app.log</code> ou <code>mail.log</code>), então não faz nenhum sentido&#8201;&#8212;&#8201;e nem sequer é possível&#8201;&#8212;&#8201;que eles sejam criados no mesmo diretório.</p>
</div>
<div class="paragraph">
<p>Para cenários como esses, <em>subPaths</em> são ideais. Esse atributo permite que um mesmo volume seja utilizado múltiplas vezes em um único pod, ou que variáveis e expressões regulares sejam utilizada para customizar o caminho de escrita e leitura em um volume. Vamos testar esse recurso nesta atividade.</p>
</div>
<div class="olist loweralpha">
<ol class="loweralpha">
<li>
<p>Crie um novo PVC, <code>events</code>, com tamanho de 100Mi. Ele será utilizado para armazenar os eventos gerados por múltiplos pods.</p>
<div class="paragraph">
<p>A seguir, edite o deployment <code>author</code> e faça com que o diretório <code>/log</code> seja montado sob o PV criado no passo anterior. Do ponto de vista do servidor NFS, os arquivos criados por um pod devem ser armazenados dentro do diretório <code>/pods/${EVENT_PV}/${POD_NAME}</code>, sendo <code>${EVENT_PV}</code> o PersistentVolume criado automaticamente pelo PVC <code>events</code> e <code>${POD_NAME}</code> o <em>hostname</em> do pod em questão.</p>
</div>
<div class="paragraph">
<p>Finalmente, entre em cada um dos pods do deployment e crie um arquivo de eventos no caminho <code>/log/app.log</code> com qualquer conteúdo. Verifique o funcionamento de sua configuração.</p>
</div>
<details>
<summary class="title">Visualizar resposta</summary>
<div class="content">
<div class="paragraph">
<p>A atividade é relativamente complexa; vamos lá. Para criar o PVC <code>events</code>, podemos utilizar o arquivo YAML do PVC <code>novel</code> como base e editar os atributos-chave via <code>sed</code>. Veja:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># cat novel.yaml | sed 's/novel/events/' | sed 's/200/100/' | kubectl apply -f -</strong>
persistentvolumeclaim/events created</pre>
</div>
</div>
<div class="paragraph">
<p>A seguir, edite o deployment <code>author</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl edit deploy author</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Para escrever em um diretório com o nome do pod, primeiro precisamos determinal qual é ele&#8201;&#8212;&#8201;felizmente, isso pode ser feito através de <em>Downward API environment variables</em>, documentadas em <a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/" class="bare">https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/</a> . Na seção <code>.spec.template.spec.containers.env</code>, insira o excerto abaixo.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">env</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POD_NAME</span>
  <span class="na">valueFrom</span><span class="pi">:</span>
    <span class="na">fieldRef</span><span class="pi">:</span>
      <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">fieldPath</span><span class="pi">:</span> <span class="s">metadata.name</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Com a variável <code>${POD_NAME}</code> definida, podemos utilizá-la na seção <code>.spec.template.spec.containers.volumeMounts</code>, especificando o atributo <code>subPathExpr</code>. Esta funcionalidade é documentada em <a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath-with-expanded-environment-variables" class="bare">https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath-with-expanded-environment-variables</a> . Utilize o excerto abaixo:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">volumeMounts</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/log</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">log-dir</span>
  <span class="na">subPathExpr</span><span class="pi">:</span> <span class="s">$(POD_NAME)</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Finalmente, na seção <code>.spec.template.spec.volumes</code>, adicione o PVC criado anteriormente.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="yaml"><span class="na">volumes</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">log-dir</span>
  <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
    <span class="na">claimName</span><span class="pi">:</span> <span class="s">events</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Saia e salve o arquivo. Os pods do deployment serão encerrados e recriados&#8201;&#8212;&#8201;devido à estratégia de deployment <code>Recreate</code>. Após algum tempo, ambos deverão estar ativos:</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl get pod -l app=author</strong>
NAME                      READY   STATUS    RESTARTS   AGE
author-5dbfb76865-p9m2z   1/1     Running   0          35s
author-5dbfb76865-qw7r5   1/1     Running   0          35s</pre>
</div>
</div>
<div class="paragraph">
<p>Entre em cada um dos pods acima e crie o arquivo <code>/log/app.log</code>, com um conteúdo qualquer.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it author-5dbfb76865-p9m2z -- /bin/sh -c 'echo event1 > /log/app.log'</strong></pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl exec -it author-5dbfb76865-qw7r5 -- /bin/sh -c 'echo event2 > /log/app.log'</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Vamos ver o que aconteceu com o diretório compartilhado via NFS.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># ls /pods</strong>
default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632
default-novel-pvc-49b725f6-9d5a-4d5a-927a-2e8044e232c3</pre>
</div>
</div>
<div class="paragraph">
<p>Temos a nova pasta <code>default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632</code>, que referencia o PVC <code>events</code>. Dentro dela, foram criadas duas pastas&#8201;&#8212;&#8201;com o nome dos pods pertencentes ao deployment <code>author</code>, como esperaríamos.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># ls -1 /pods/default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632/</strong>
author-5dbfb76865-p9m2z
author-5dbfb76865-qw7r5</pre>
</div>
</div>
<div class="paragraph">
<p>Listando recursivamente esse diretório, notamos que cada um desses diretórios possui um arquivo <code>app.log</code> dentro de si.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># ls -R /pods/default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632/</strong>
/pods/default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632/:
author-5dbfb76865-p9m2z  author-5dbfb76865-qw7r5

/pods/default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632/author-5dbfb76865-p9m2z:
app.log

/pods/default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632/author-5dbfb76865-qw7r5:
app.log</pre>
</div>
</div>
<div class="paragraph">
<p>Usando um <em>loop</em> <code>for</code> combinado com o comando <code>find</code>, é trivial imprimir o conteúdo desses arquivos na tela e constatar que seu conteúdo é o mesmo que foi inserido dentro do contexto dos pods.</p>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># for file in $( find /pods/default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632/ -type f -print ); do echo -e "\n$file\n$( cat $file )"; done</strong>

/pods/default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632/author-5dbfb76865-qw7r5/app.log
event2

/pods/default-events-pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632/author-5dbfb76865-p9m2z/app.log
event1</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Finalmente, remova todos os objetos criados nesta sessão para liberar recursos do <em>cluster</em>:</p>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete deploy,pvc,pv --all</strong>
deployment.apps "author" deleted
deployment.apps "nfs-client-provisioner" deleted
persistentvolumeclaim "events" deleted
persistentvolumeclaim "novel" deleted
persistentvolume "pvc-2bf969eb-d07e-4a6c-b9a0-1c0c979d6632" deleted
persistentvolume "pvc-49b725f6-9d5a-4d5a-927a-2e8044e232c3" deleted</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete sc --all</strong>
storageclass.storage.k8s.io "nfs-client" deleted</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete role leader-locking-nfs-client-provisioner ; kubectl delete rolebindings.rbac.authorization.k8s.io leader-locking-nfs-client-provisioner</strong>
role.rbac.authorization.k8s.io "leader-locking-nfs-client-provisioner" deleted
rolebinding.rbac.authorization.k8s.io "leader-locking-nfs-client-provisioner" deleted</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre><strong># kubectl delete clusterrole nfs-client-provisioner-runner ; kubectl delete clusterrolebinding run-nfs-client-provisioner</strong>
clusterrole.rbac.authorization.k8s.io "nfs-client-provisioner-runner" deleted
clusterrolebinding.rbac.authorization.k8s.io "run-nfs-client-provisioner" deleted</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Aviso"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>ENTREGA DA TAREFA</strong></p>
</div>
<div class="paragraph">
<p>Para que seja considerada entregue você deve anexar a esta atividade no AVA uma imagem (nos formatos .png ou .jpg) do terminal com a saída do comando <code>ls</code> que mostra que dos arquivos <code>app.log</code> <strong>diferentes</strong> foram criados em pastas nos diretórios <code>/pods/*events*/author*</code>. Isto irá comprovar o funcionamento da utilização concorrente de volumes no <em>cluster</em>.</p>
</div>
<div class="paragraph">
<p>Utilize como referência a saída de comando mostrada na atividade 7.3.3 (a) deste roteiro.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Última atualização 2023-06-24 10:08:40 -0300
</div>
</div>
</body>
</html>
